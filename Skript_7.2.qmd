---
title: "Korrelation & Regression"
author: "Stephanie Geise"
toc: true
number-sections: true
highlight-style: pygments
execute: 
  warning: false
format:
  html:
    code-fold: false
    code-line-numbers: true
    code-annotations: hover
editor_options: 
  chunk_output_type: inline
---

![The Highway as regression curve, Bild generiert von Midjourney](Bilder/zemkipatrick_comic_style_a_barplot_as_city_with_a_regression_cu_aef459c9-87b7-4762-b34b-88abbbbf96ff.png)

Zur Analyse von Zusammenh√§ngen bei zwei metrischen Variablen gibt es verschiedene statistische Verfahren. In diesem Teilkapitel schauen wir uns die *Korrelationsanalyse* sowie die *einfache lineare Regression* an.

# Die Korrelationsanalyse

Die Korrelationsanalyse ist eine schnelle und einfache statistische Methode, um den Zusammenhang zwischen zwei oder mehr metrischen Variablen zu untersuchen. Ziel der Korrelationsanalyse ist, die Richtung (positiv oder negativ) und die St√§rke eines linearen Zusammenhangs zwischen den Variablen zu bestimmen. Als statistischer Kennwert wird ein *Korrelationskoeffizient*, zum Beispiel der *Pearson-Korrelationskoeffizient* (auch Pearson's r) ermittelt. Der Korrelationskoeffizient gibt an, wie sehr die Werte einer Variable mit den Werten einer anderen Variable zusammenh√§ngen, d.h. *korrelieren*. Er nimmt Werte zwischen -1 und 1 an. Ein *hoher* Korrelationskoeffizient (nahe 1 oder -1) deutet auf einen *starken Zusammenhang* hin, w√§hrend ein *niedriger* Koeffizient (nahe 0) auf einen *schwachen* oder *keinen Zusammenhang* hindeutet. Zudem deutet ein *positiver* Korrelationskoeffizient auf einen *positiven Zusammenhang* hin, w√§hrend ein *negativer* Koeffizient auf einen *negativen Zusammenhang* verweist.

Der Zusammenhang zwischen den untersuchten Variablen wird dabei analysiert, *ohne* dass eine Richtung spezifiziert wird - es muss also nicht die eine Variable als unabh√§ngig, und die andere als abh√§ngige Variable festgelegt werden. Die Korrelationsanalyse kann vielf√§ltig eingesetzt werden. Sie ist allerdings etwas empfindlich gegen√ºber Ausrei√üern und setzt einen linearen Zusammenhang voraus.

Vorsicht! Lassen Sie sich nicht verwirren: Der *Pearson-Korrelationskoeffizient* und die *Rangkorrelation nach Pearson* sind unterschiedliche Kennwerte, die nur √§hnlich klingen. Der *Pearson-Korrelationskoeffizient* misst den linearen Zusammenhang zwischen den tats√§chlichen Messwerten, w√§hrend die *Rangkorrelation nach Pearson* (auch als *Spearman-Korrelationskoeffizient* bezeichnet) den monotonen Zusammenhang zwischen den R√§ngen der Daten untersucht. Daher ist die Rangkorrelation besser geeignet, wenn der Zusammenhang *nicht-linear* ist oder wenn *Ausrei√üer in den Daten* vorhanden sind. Nun soll es aber um die Analyse eines *linearen Zusammenhangs* zwischen zwei metrischen Variablen gehen. Dazu widmen wir uns nun der Korrelationsanalyse mittels Pearson-Korrelationskoeffizient.

Mit dieser Methode wollen wir untersuchen, ob es einen Zusammenhang zwischen der metrischen Variable t√§gliche Fernsehnutzung in Minuten (lm02) sowie der quasi-metrischen Variable Vertrauen ins Fernsehen (pt09) gibt. Wir vermuten, dass es einen positiven Zusammenhang gibt: Wer ein h√∂heres Vertrauen in das Fernsehen hat, schaut auch mehr fern.

Um das zu pr√ºfen, m√ºssen wir zun√§chst wieder unsere Pakete und Allbus-Daten laden und aufbereiten. Los geht's!

## Laden der Pakete und des Datensatzes

```{r}
if(!require("pacman")) {install.packages("pacman");library(pacman)}
p_load(tidyverse, haven, dplyr) 
daten = haven::read_dta("Datensatz/Allbus_2021.dta")
```

Dann erzeugen wir einen neuen Teildatensatz, benennen die ben√∂tigten Variablen in "TV_Konsum" und "TV_Vertrauen" um und selektieren die fehlende Werte (z.B. -9=Keine Angabe) mittels Filter-Funktion.

## Erzeugen eines Teildatensatzes, Umbenennen der Variablen und Filtern der F√§lle

```{r}
daten_neu <- daten %>%
  rename(TV_Konsum = lm02)%>%
  rename(TV_Vertrauen= pt09)%>%
  filter(between(TV_Konsum, 0, 1500))%>%
  filter(between(TV_Vertrauen, 1, 7))
```

Nun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:

```{r}
korrelation <- daten_neu %>% 
  summarize(correlation = cor(TV_Konsum, TV_Vertrauen, use = "complete.obs"))
print(korrelation)
```

Schauen wir uns den (sehr √ºbersichtlichen) Output an: Wie erhalten einen Korrelationskoeffizient von 0,113903. Das bedeutet, dass zwischen den beiden Variablen TV_Konsum und TV_Vertrauen ein schwacher positiver linearer Zusammenhang besteht. H√∂here Werte in einer Variable gehen also tendenziell mit h√∂heren Werten in der anderen Variable einher. Jedoch ist die Korrelation relativ niedrig, was darauf hindeutet, dass der Zusammenhang zwischen den beiden Variablen nicht besonders stark ist.

Um die Korrelation zwischen den Variablen "TV_Konsum" und "TV_Vertrauen" zu visualisieren, k√∂nnen wir mit dem ggplot2-Paket ein Scatterplot erstellen:

## Visualisierung der Korrelation mit Hilfe eines Scatterplots

```{r}
scatterplot <- ggplot(daten_neu, aes(x = TV_Konsum, y = TV_Vertrauen)) +
  geom_point(color = "blue") +
  labs(x = "TV Konsum", y = "TV Vertrauen", title = "Korrelation zwischen TV Konsum und Vertrauen")
print(scatterplot)
```

Hier k√∂nnen wir den oben bereits ermittelten Befund noch einmal grafisch inspizieren. Der leichte positive Zusammenhang zwischen dem Vertrauen in die Insitution Fernsehen und der t√§glichen Fernsehnutzung zeigt sich ganz sch√∂n.

F√ºhren Sie zur √úbung nun noch eine Korrelationsanalyse durch, um den Zusammenhang zwischen der t√§glichen Fernsehnutzung in Minuten (umbenannt in "TV_Konsum") und der quasi-metrisch gemessenen Einsch√§tzung, ob die Entwicklung der Kriminalit√§t in Deutschland zu- oder abgenommen hat (cf03; 1=hat stark zugenommen; 5=hat stark abgenommen). Entsprechend der Kultivierungsforschung k√∂nnen wir vermuten, dass es einen negativen Zusammenhang geben sollte: Mehr Fernsehkonsum sollte mit einer negativen Kriminalit√§tsprognose korrelieren. Dazu m√ºssen Sie zuerst wieder einen Teildatensatz mit den ben√∂tigten Variablen erzeugen, diese ggf. umbennen und filtern. Dann ermitteln Sie die Korrelation beider Variablen.

## Erzeugen eines Teildatensatzes, Umbenennen der Variablen und Filtern der F√§lle

```{r}
daten_neu2 <- daten %>%
  rename(TV_Konsum = lm02)%>%
  rename(Kriminalit√§tsprognose= cf03)%>%
  filter(between(TV_Konsum, 0, 1500))%>%
  filter(between(Kriminalit√§tsprognose, 1, 5))
```

Nun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:

## Berechnung und Ausgabe des Korrelationskoeffizienten

```{r}
korrelation <- daten_neu2 %>% 
  summarize(correlation = cor(TV_Konsum, Kriminalit√§tsprognose, use = "complete.obs"))
print(korrelation)
```

Betrachten wir den Output: Der Korrelationskoeffizient von -0,196 deutet auf einen schwachen negativen linearen Zusammenhang zwischen der "t√§glichen Fernsehnutzung in Minuten" und der Einsch√§tzung zur Entwicklung der Kriminalit√§t in Deutschland hin.

ACHTUNG! Zur Interpretation m√ºssen Sie nun aber auch ber√ºcksichtigen, wie die Werte der Variable gelabelt sind: Ein h√∂herer Wert bedeutet, dass man eine Abnahme der Kriminalit√§tsentwicklung vermutet (1=hat stark zugenommen; 5=hat stark abgenommen). Der negative Korrelationskoeffizient bedeutet dann, dass h√∂here Werte in der "t√§glichen Fernsehnutzung in Minuten" tendenziell mit niedrigeren Werten in der Einsch√§tzung zur Kriminalit√§tsentwicklung korrelieren. Oder anders gesagt: Personen, die mehr Fernsehen schauen, tendieren eher zur Annahme, dass die Kriminalit√§t in Deutschland weniger zugenommen oder sogar abgenommen hat.

So oder so ist zu beachten, dass der Korrelationskoeffizient nur den linearen Zusammenhang erfasst und keine Aussagen zur Kausalit√§ten des Zusammenhangs macht. Korrelation ist nicht Kausalit√§t!

Zur grafischen Illustration erstellen wir nun wieder ein Scatterplot:

## Visualisierung der Korrelation mit Hilfe eines Scatterplots

```{r}
scatterplot <- ggplot(daten_neu2, aes(x = TV_Konsum, y = Kriminalit√§tsprognose)) +
  geom_point(color = "red") +
  labs(x = "TV Konsum", y = "Kriminalit√§tsprognose", title = "Korrelation zwischen TV Konsum und Kriminalit√§tsprognose")
print(scatterplot)
```

# Die Regressionsanalyse: Analyselogik, Ziel und Einsatzgebiete

Die *lineare Regression* untersucht den Zusammenhang zwischen einer *abh√§ngigen Variable* und mindestens einer *unabh√§ngigen Variable*. Sie versucht, eine mathematische Beziehung zwischen den Variablen zu modellieren, die durch eine Linie (in einfachen linearen Regressionen) oder eine Ebene (in multiplen linearen Regressionen) repr√§sentiert wird. Die dahinterliegende "mathematische Idee" der linearen Regression besteht darin, die bestm√∂gliche Anpassung der Daten an das Modell zu erreichen, um die Vorhersage der abh√§ngigen Variable basierend auf den unabh√§ngigen Variablen zu erm√∂glichen. Dazu wird ein *Regressionskoeffizient* f√ºr jede unabh√§ngige Variable gesch√§tzt, um ihren Einfluss auf die abh√§ngige Variable zu quantifizieren.

Ziel der Regressionsanalyse ist es also, die Beziehung zwischen einer *abh√§ngigen Variable* (auch erkl√§rte Variable, Regressand oder Prognosevariable genannt) und einer oder mehreren *unabh√§ngigen Variablen* (oft auch erkl√§rende Variable, Regressor oder Pr√§diktorvariable) zu analysieren, um Zusammenh√§nge quantitativ zu beschreiben und zu erkl√§ren und/oder Werte der abh√§ngigen Variable mit Hilfe der unabh√§ngige Variable (des Pr√§diktors) zu prognostizieren. Mit Hilfe der Regressionsanalyse k√∂nnen drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen der unabh√§ngigen und der abh√§ngigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie ver√§ndert sich die abh√§ngige Variable bei einer √Ñnderung der unabh√§ngigen Variablen? 3) Prognose: K√∂nnen die Messwerte der abh√§ngigen Variable durch die Werte der unabh√§ngigen Variable vorhergesagt werden?

# Die Einfache lineare Regression

Die *einfache lineare Regression* wird angewandt, wenn gepr√ºft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen einer abh√§ngigen metrischen Variable und einer unabh√§ngigen metrischen Variable besteht. Da sie zwei metrische Variablen integriert, wird sie auch als *bivariate Regression* bezeichnet. In diesem Teilkapitel lernen wir die *einfache lineare Regression* auf Grundlage der Allbus-Daten kennen. In der n√§chsten Sitzung gehen wir dann n√§her auf die notwendige Pr√ºfung der *Voraussetzungen einer Regressionsanalyse* ein und lernen auch noch die *multiple lineare Regression* kennen.

Um die Durchf√ºhrung der linearen Regression an einem Beispiel nachzuvollziehen, stellen wir zun√§chst eine gerichtete Hypothese auf, die den Einfluss einer unabh√§ngigen metrischen Variable auf eine abh√§ngige metrische Variable spezifiziert. Hierzu schauen wir uns den Zusammenhang zwischen dem Alter (age) sowie der t√§glichen Fernsehnutzung in Minuten (lm02) an. Dazu vermuten wir: Das Alter erkl√§rt die Intensit√§t der t√§glichen Fernsehnutzung in Minuten.

![Grafik: Kann das Alter die Fernsehnutzung erkl√§ren? (Picture generated by midjourney)](Bilder/zemkipatrick_comic_style_young_and_old_people_watching_tv_widef_92db5ce1-7d64-4c5b-b05c-8257c92de958.png)

## Vorbereitung und Laden der Daten

Wie immer besteht der erste Schritt nun darin, die ben√∂tigten Pakete sowie den Datensatz zu laden. F√ºr die lineare Regression kommen zwei neue Pakete hinzu: Wir laden das Paket broom, um die normale Ausgabe der Funktion lm (f√ºr die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu k√∂nnen sowie das Paket performance, dass uns sp√§ter zus√§tzlich einige Indikatoren ausgibt. Das Paket "see" kommt au√üerdem dazu, weil es uns eine toolbox f√ºr die Visualisierung der Zusammenh√§nge bereitstellt.

### Laden der ben√∂tigten Pakete

```{r}
if(!require("pacman")) {install.packages("pacman");library(pacman)}
p_load(tidyverse, haven, dplyr, broom, lm.beta, performance, see) 
```

### Laden des Datensatzes

```{r}
daten = haven::read_dta("Datensatz/Allbus_2021.dta")
```

### Erzeugen eines Teildatensatzes & Umbenennen der Variablen

F√ºr den Teildatensatz bennenen wir die Variablen lm02 und age um und filtern die missings heraus (z.B. -9=Keine Angabe):

```{r}
daten <- daten %>%
  rename(Alter = age)%>%
  rename(TV_Konsum = lm02)%>%
  filter(between(Alter, 18, 100))%>%
  filter(between(TV_Konsum, 0, 1500))


theme_set(theme_minimal()) # <1>

options(scipen = 999)  # <2>
```

1.  Visualisierungshintergrund der Grafiken in ggplot festlegen
2.  Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen

## Ziel der Analyse

Mit Hilfe der Regression wollen wir nun die oben formulierte Annahme pr√ºfen, dass das Alter die t√§glichen Fernsehnutzung in Minuten (umbenannt in TV_Konsum) erkl√§ren kann. Beides sind metrische Variablen und erf√ºllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen k√∂nnen bei der Regressionsanalyse grunds√§tzlich eingesetzt werden, sie m√ºssen dann aber durch *Dummy-Coding* passend gemacht werden, dazu aber sp√§ter mehr).

Bevor wir die Regressionsanalyse durchf√ºhren, verschaffen wir uns zun√§chst einmal einen √úberblick √ºber die Daten - dazu visualisieren wir den Zusammenhang zwischen dem Alter sowie der t√§glichen Fernsehnutzung. Mit Hilfe der Grafik k√∂nnen wir auch die erste Voraussetzung pr√ºfen, n√§mlich dass es einen *linearen Zusammenhang* zwischen beiden Variablen gibt.

## Grafische Pr√ºfung der Voraussetzungen

ACHTUNG! F√ºr die Regressionsanalyse m√ºssen noch weitere Voraussetzungen gepr√ºft werden (insb. Homoskedastizit√§t der Residuen; Unabh√§ngigkeit der Residuen; Normalverteilung der Residuen; keine Ausrei√üer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungspr√ºfungen aber vorerst aus, und kommen in der n√§chsten Sitzung darauf zur√ºck (das ist sonst zu viel auf einmal).

Zur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und TV_Konsum. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Alter) und x (=TV_Konsum) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie m√∂glich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab erg√§nzen wir die Achsenbeschriftung.

### Visualisierung des Zusammenhangs mit Hilfe eines Streudiagramms

```{r }
ggplot(daten, aes(Alter, TV_Konsum)) + 
  geom_point() + 
  geom_smooth(method = lm, formula = "y ~ x") + 
  ggtitle("Zusammenhang der Variablen Alter und TV_Konsum") + 
  xlab("TV_Konsum") + ylab("Alter")
```

### Interpretation: Was sehen wir im Streudiagramm?

Die grafische Darstellung legt uns einen positiven und linearen Zusammenhang zwischen Alter und Fernsehnutzung nahe: mit zunehmendem Alter steigt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erf√ºllt.

## Durchf√ºhrung der einfachen linearen Regression √ºber die Funktion lm

Ob diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der *einfachen linearen Regression* pr√ºfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht f√ºr "linear model". In den Klammern benennen wir zun√§chst die abh√§ngige Variable (hier: TV_Konsum), dann kommt eine Tilde (d.h. "wird definiert durch") und der Bezug auf unsere unabh√§ngige Variable (hier: Alter). Die Schreibweise y \~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (TV_Konsum) abh√§ngig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll. Zum Schluss lassen wir uns das Modell ausgeben.

Der Modelloutput von lm √§hnelt dem der schon behandelten Hypothesentests; enth√§lt aber noch weitere Eckdaten wie die Effektst√§rke, das Signifikanzniveau oder die Erkl√§rungsst√§rke des Modells.

### Einfache lineare Regression mit lm (=linear models)

```{r}
model <- lm(TV_Konsum ~ Alter, data = daten) 
print(model)
```

Da dieser Output sehr, sehr sparsam und f√ºr uns noch wenig aussagekr√§ftig ist, erg√§nzen wir ihn nun mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das "Herzst√ºck" unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten √úberblick √ºber unser Regressionsmodell:

```{r}
summary(model)
```

### Interpretation des Outputs: Was sehen wir in der Ausgabe?

Unter *Call* wird zun√§chst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes "daten" die abh√§ngige Variable "TV_Konsum" durch die unabh√§ngige Variable "Alter" zu erkl√§ren.

Unter *Resdiuen* erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.

Das *Intercept* definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert f√ºr y, wenn x den Wert 0 annimmt).

Die *Estimates* sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (k√∂nnten).

Mit *St.error* wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.

Der *t-value* gibt den t-Wert des Modells an (Koeffizient / Standardfehler)

Der *p-value* ist f√ºr uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs

Auch beide *R-Werte* (*R2*, *Adjusted R2*) sind von zentraler Bedeutung f√ºr die Interpretation: *R2* gibt uns die erkl√§rte Gesamtvarianz des Modells der abh√§ngigen Variable an, also die "Erkl√§rungskraft" der unabh√§ngigen Variable Alter auf die abh√§ngige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir k√∂nnen hier dann daraus lesen, dass das Alter (8,8) Prozent der Varianz der TV-Nutzung erkl√§rt. Das ist nicht super viel, aber auch nicht nichts. Wir k√∂nnen daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Intensit√§t der Fernsehnutzung mitbestimmen m√ºssen. √úbrigens: Das R2 k√∂nnte theoretisch maximal den Wert 1 annehmen, dann h√§tten wir eine 100% Erkl√§rung der abh√§ngigen Variable durch die unabh√§ngige Variable (das kommt in der Realit√§t aber fast nicht vor).

Wie der Name schon sagt bezeichnet *Adjusted R2* die Anpassung des Modells, wobei f√ºr die Anzahl der aufgenommenen Variablen korrigiert wird ("Strafterm f√ºr viele aufgenommene Variablen"). Das Adjusted R2 ist daher immer schlechter als R2.

Auch die *F-Statistik* ist wichtig: Sie gibt uns n√§mlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)

### Inhaltliche Interpretation des Outputs: Was bedeutet das jetzt also alles?

Der Output zeigt uns: Das Alter hat einen positiven Einfluss auf die t√§gliche Fernsehnutzung in Minuten. Je √§lter ein Nutzer ist, desto mehr nutzt er das Fernsehen. Die Regressionsanalyse l√§sst dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabh√§ngige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabh√§ngige Variable TV-Konsum um 2,19 Messeinheiten (hier: Minuten) zu. Dieser Zusammenhang ist mit p \> .05 statistisch signifikant.

::: callout-tip
## Wie gebe ich die Ergebnisse korrekt an?

Die Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. F√ºr die Angabe im Text wird folgendes gebraucht:

‚úÖ den R^2^-Wert (das Bestimmungskoeffizient)

‚úÖ den F-Wert (auch als F-Statistik bezeichnet)

‚úÖ die Freiheitsgrade in Klammern

‚úÖ den p-Wert

Das Format ist normalerweise:

> ***Beispiel:*** Das Alter beeinflusst die Dauer der Fernsehnutzung, *R^2^* = .08, *F*(1, 4927) = 474.1, *p* = .000.
:::

### Vorhersage von Werten auf Basis des Modells

Da bei der Regression eine lineare Funktion gesch√§tzt wird, k√∂nnen wir anhand der b-Werte und des Intercepts auch Vorhersagen f√ºr bestimmte F√§lle treffen. Das ist eine der *coolen Superkr√§fte* der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repr√§sentativ ist und das Modell sowie die Parameter signifikant sind).

Zur Prognose von erwarteten Werten der abh√§ngigen Variable (TV_Konsum) auf Basis von gegebenen Werten der unabh√§nigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:

### Beispiel 1: Vorhersage f√ºr die t√§gliche Internetnutzung

Wir lassen uns mit Hilfe unseres Modells zun√§chst die t√§gliche Internetnutzung in Minuten bei einem Alter von 25 und 75 vorhersagen.

```{r}
predict.lm(model, data.frame(Alter = 25))
predict.lm(model, data.frame(Alter = 75))
```

### Interpretation des Outputs: Was sehen wir in der Ausgabe?

Eine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von 119 Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von 229 Minuten auf.

Das geht nat√ºrlich auch kombiniert in einem Befehl, dann m√ºssen wir aber mit c() einen combine-Befehl einf√ºgen:

## Beispiel 2: Vorhersage f√ºr die t√§gliche Internetnutzung

In unserem zweiten Beispiel betrachten wir andere Altersgruppen. Wir lassen uns hier mit Hilfe unseres Modells die t√§gliche Internetnutzung in Minuten bei einem Alter von 20, 30, 40, 50, 60 und 70 vorhersagen.

```{r}
predict.lm(model, data.frame(Alter = c(20, 30, 40, 50, 60, 70)))
```

### Vorhersage und Residuen berechnen

Die Prognose-Leistung unseres Regressionsmodels k√∂nnen wir auch auf den gesamten Datensatz anwenden - dann bekommen wir f√ºr jeden Fall im Datensatz den prognostizierten Wert der abh√§ngigen Variable TV-Konsum ausgegeben. Dazu k√∂nnen wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen TV-Konsum von (X) Minuten.

Weil wir im Datensatz aber n=4929 F√§lle haben (und die Ausgabe sonst zu un√ºbersichtlich wird), begrenzen ich die Ausgabe auf die ersten 100 Zeilen (F√§lle) des Datensatzes, indem ich zus√§tzlich die Funktion head() verwende, und die Anzahl der gew√ºnschten F√§lle in der Klammer mit 100 festlege:

### Vorhersagewerte f√ºr jede Beobachtung anzeigen

```{r }
head(fitted(model), 100)
```

### Interpretation des Outputs: Was sehen wir in der Ausgabe?

In der Ausgabe kann ich nun die laut Modell prognostizierte Fernsehnutzung f√ºr meine Befragten entsprechend ihres Alters sehen - Befragte(r) 10 hat so z.B. eine prognostizierte Fernsehnutzung von 189 Minuten.

Nun haben wir im Rahmen unserer Befragung die TV-Nutzung der Befragten aber ja schon erhoben. Wozu dient diese Prognose dann? Ganz einfach: √úber die Ausgabe der prognostizierten Werte und der Residuen k√∂nnen wir sehen, wie hoch die *Abweichung der Modellprognose* ist, d.h. wie sehr die empirisch beobachteten Werte unserer F√§lle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:

```{r Residuen anzeigen}
head(residuals(model), 100)
```

### Interpretation des Outputs: Was sehen wir in der Ausgabe?

F√ºr unseren Fall Nummer 65 betr√§gt die Abweichung der Prognose von der Beobachtung -92 Minuten. Das diese Abweichung sehr gro√ü ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine gro√üe Spannweite haben und unser R2 mit 8 Prozent nicht besonders gro√ü ist.

### Vorhersage und Residuen grafisch darstellen

Um das grafisch gegen√ºberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen "vorhersage" und "residuen" ab.

### Berechnung der Vorhersagewerte und Residuen f√ºr zus√§tzliche Plots

```{r }
daten$vorhersage <- predict(model) 
daten$residuen <- as.numeric(residuals(model))
```

Und dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen l√§sst:

### Beobachtete und vorhergesagte Werte sowie Residuen gemeinsam plotten

```{r }
ggplot(daten, aes(Alter, TV_Konsum)) + 
  geom_point(aes(color = residuen)) + # <1> 
  scale_color_gradient2(low = "blue", mid = "white", high = "red") + # <2> 
  guides(color = "none") +  # <3> 
  geom_point(aes(y = vorhersage), shape = 1) + # <4> 
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE, linewidth = 0.5, color = "black") + # <5> 
  geom_segment(aes(xend = Alter, yend = vorhersage), alpha = .2) + # <6> 
  ggtitle("Vorhergesagte Werte und Residuen f√ºr Alter und Fernsehnutzung") + # <7> 
  xlab("Alter") + ylab("t√§gliche Fernsehnutzung (Minuten)") # <8> 
```

1.  Festlegung der Farbmarkierung f√ºr die Residuen (Punkte sind tats√§chliche Werte, Linien die Residuen, Farbe gibt Gr√∂√üe der Abweichung an)
2.  Festlegung der Farbe f√ºr die Residuen
3.  Unterdr√ºckt eine Legende an der Seite (ist obligatorisch)
4.  gibt die vorhergesagten Punkte auf der Regressionsgeraden aus
5.  gibt die Regressionsgerade als Linie aus
6.  zeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein
7.  Titel
8.  Achsen-Beschriftung

```{r}
vorhersagen <- predict(model)
residuen <- residuals(model)

plot(vorhersagen, residuen, # <1> 
     xlab = "Vorhersagen", ylab = "Residuen", # <1> 
     main = "Residualplot", pch = 16, col = "blue") # <1> 
abline(h = 0, col = "red", lty = 2)  # <2> 

```

1.  Residualplot erstellen
2.  Hinzuf√ºgen einer Linie bei y = 0 f√ºr Referenz

### Standardisierung der B-Koeffizienten ("beta-Koeffizienten")

Neben den normalen Regressionskoeffizienten b kann man auch die *standardisierten Koeffizienten beta* berechnen. Die standardisierten beta-Koeffizienten sind n√ºtzlich, weil sie die Skalierung der einzelnen Messwerte "herausrechnet", wodurch unterschiedlich skalierte Variablen vergleichbar werden.

Um uns die standardisierten beta-Koeffizienten ausgeben zu lassen, k√∂nnen wir auf das Paket lm.beta mit der gleichnamigen Funktion zur√ºckgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern erg√§nzt um die Funktion lm.beta(), die daf√ºr sorgt, dass wir im Output unten eine zus√§tzliche Spalte erhalten, in der die *standardisierten B-Koeffizienten* angezeigt werden. Diese sind als standardisierte "Regressionsgewichte" zu interpretieren - je h√∂her der Wert, desto st√§rker der erkl√§rende Beitrag der Variable.

Das ist nat√ºrlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Pr√§diktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erkl√§rungsbeitrag untereinander vergleichen.

```{r}
summary(lm.beta(model)) 
```

## Und zum Abschluss noch ein paar n√ºtzliche Zusatzfunktionen zur sch√∂neren Ergebnisdarstellung durch das Paket broom:

### Funktion zur Modellzusammenfassung als data frame

```{r }
tidy(lm.beta(model))
```

### Funktion, um weitere Modellstatistiken in einem Befehl zu berechnen

```{r }
glance(model)
```

### Funktion, um Rohdaten um Modellvorhersagen zu erweitern

```{r}
augment(model)
```

::: callout-note
## Literatur und Beispiele aus der Praxis

Wir empfehlen euch folgende Lehrb√ºcher, falls ihr weiterf√ºhrende Informationen braucht.

> üìñ Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. [Link](https://suche.suub.uni-bremen.de/peid=B68436977&LAN=DE&CID=7699632&index=L&Hitnr=1&dtyp=D&rtyp=a&Exemplar=1)

> üìñ D√∂ring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. [Link](https://suche.suub.uni-bremen.de/peid=B77441304&LAN=DE&CID=7699632&index=L&Hitnr=1&dtyp=D&rtyp=a&Exemplar=1)

Hier findet ihr ein Beispiel aus der Forschungspraxis:

> üî¨ Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. [Link](https://journals.sagepub.com/doi/pdf/10.1177/1077699013482906?casa_token=ZaoVG0aHKusAAAAA:xsH-XzhmHjTPFafhnQkIQ5MaEvECLcI1WPrCn7uSaBCLQSgfgXHlSm2tk7ln9XZH-B_hrHHQTdqBVw)
:::
