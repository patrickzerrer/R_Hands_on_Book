---
title: "Pr√ºfung der Voraussetzungen bei der linearen Regression"
author: "Stephanie Geise"
toc: true
number-sections: true
highlight-style: pygments
execute: 
  warning: false
  message: false
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
editor_options: 
  chunk_output_type: inline
---

![Das Testen der Maschine, Bild generiert von Midjourney](Bilder/zemkipatrick_comic_style_a_femal_person_is_supervising_a_machin_5565b4d9-8bdc-43d3-b87e-0f2162bb5460.png)

In diesem Teilkapitel gehen wir (wie angek√ºndigt) n√§her auf die Pr√ºfung der *Voraussetzungen einer Regressionsanalyse* ein. Im folgenden Teilkapitel lernen wir dann die *multiple lineare Regression* kennen, die es erlaubt, Zusammenh√§nge zwischen *mehreren* x-Variablen und einer y-Variablen zu analysieren.

# Data Management

Zun√§chst laden wir wieder die Pakete des `tidyverse` und das Pakete `broom`, um die normale Ausgabe der Funktion `lm` (f√ºr die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu k√∂nnen. Au√üerdem laden wir das Paket `performance`, dass wir f√ºr die Voraussetzungspr√ºfung brauchen, sowie die Pakete `lmtest` und `sandwich`, mit der wir fehlende Voraussetzungen korrigieren k√∂nnen (siehe unten).

```{r}
if(!require("pacman")) {install.packages("pacman");library(pacman)} # <1>
p_load(tidyverse, lm.beta, lmtest, performance, easystats, haven, broom, see, haven, sandwich) # <1>

theme_set(theme_minimal()) # <2>
options(scipen = 999) # <3>
```

1.  Laden der notwendigen Pakete
2.  Visualisierungshintergrund der Grafiken in ggplot festlegen
3.  Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen

Die Regression rechnen wir wieder auf Basis des ALLBUS-Datensatzes, die wir im letzten Schritt laden.

```{r}
daten = haven::read_dta("Datensatz/Allbus_2021.dta")
```

::: {.callout-note collapse="true" icon="false"}
# Erinnerung: Einfache lineare Regression

Wir stellen eine einfache lineare Regression mit den Variablen Alter als UV und TV-Nutzung als AV mit lm (=linear models) auf.

## Data Management

F√ºr den Teildatensatz bennenen wir die Variablen `lm02` und `age` um und filtern die missings heraus (z.B. -9=Keine Angabe):

```{r}
daten <- daten %>%
  rename(Alter = age)%>%
  rename(TV_Konsum = lm02)%>%
  filter(between(Alter, 18, 100))%>%
  filter(between(TV_Konsum, 0, 1500))
```

## Modellierung der linearen Regression:

Dann erstellen wir das linear model:

```{r}
model <- lm(TV_Konsum ~ Alter, data = daten) 
summary(model) # <1> 
summary(lm.beta(model)) # <2> 
```

1.  klassischer Output mit relevanten Kennzahlen
2.  klassischer Output mit relevanten Kennzahlen erweitert um standardisierte beta-Koeffizienten

Mit diesem Regressionsmodell haben wir √ºbepr√ºft, ob das Alter die Fernsehnutzung erkl√§ren kann. Im Output sehen wir, dass das Alter einen signifikanten positiven Einfluss auf die Internetnutzung. Je √§lter eine Person ist, desto mehr nutzt er/sie das Fernsehen. Die Regressionsanalyse l√§sst dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabh√§ngige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabh√§ngige Variable TV-Nutzung um "den Estimate-Wert" in Messeinheiten (hier: -2.19 Minuten) zu. Dieser Zusammenhang ist mit p \>.05 statistisch signifikant.

Soweit die Wiederholung. Beginnen wir nun mit der Pr√ºfung der *Voraussetzungen einer Regressionsanalyse*. Vielleicht wundern ihr euch, warum wir die Voraussetzungen erst im zweiten Schritt pr√ºfen? Da habt ihr Recht: Eigentlich m√ºssten wir erst die Voraussetzungen pr√ºfen, dann erst das Modell sch√§tzen. Wenn wir unser Modell aber schon gesch√§tzt haben, k√∂nnen wir Funktionen zur Pr√ºfung der Voraussetzungen auf unser gesamtes Modell anwenden (bzw. auf das entsprechende Datenobjekt "model") - und das erspart uns eine Menge "Handarbeit" mit vielen kleinen Zwischenschritten. Zum Beispiel m√ºssten wir f√ºr die Pr√ºfung der Voraussetzungen, die die Residuen betreffen, diese erst einmal berechnen und in einer neuen Variable abspeichern. Es ist also weniger Aufwand, die Voraussetzungen ex post zu pr√ºfen.
:::

# Voraussetzungspr√ºfung der linearen Regression

Bevor wir zum statistischen Teil kommen, wollen wir noch einmal Revue passieren lassen, was die wichtigsten Voraussetzungen der einfachen linearen Regression sind:

-   (quasi-)metrisches Skalenniveau
-   Linearit√§t des Zusammenhangs zwischen x und y
-   Homoskedastizit√§t der Residuen: Varianzen der Residuen der prognostizierten abh√§ngigen Variablen sind gleich - Unabh√§ngigkeit der Residuen: ansonsten Autokorrelation, die Aussagekraft reduziert
-   Normalverteilung der Residuen
-   Keine Ausrei√üer in den Daten, da schon einzelne Ausrei√üer einen sonst signifikanten Trend zunichte machen k√∂nnen (ggf. also eliminieren)

## Metrisches Skalenniveau & Linearit√§t des Zusammenhangs

Ob 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in dem letzten Skript bereits √ºberpr√ºft. F√ºr die Pr√ºfung nach der Linearit√§t des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der gesch√§tzten Regressionsgeraden erzeugt. [Hier](https://patrickzerrer.github.io/R_Hands_on_Book/Skript_7.2.html) k√∂nnt ihr noch einmal nachschauen, wie das geht.

## Homoskedastizit√§t der Residuen

Lineare Modelle setzen eine *konstante Fehlervarianz* (*Homoskedastizit√§t*) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abh√§ngigen Variablen f√ºr alle Werte des Pr√§diktors gleich sind, so dass das Modell gleich gute Vorhersagen √ºber alle Werte machen kann. Liegt Homoskedastizit√§t vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich gro√ü - unabh√§ngig davon, wie hoch oder niedrig der Wert des Pr√§diktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - *Heterokedastizit√§t der Residuen* - w√ºrde zur Ineffizienz unserer Sch√§tzung f√ºhren! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizit√§t nach oben verzerrt gesch√§tzt. Das Ergebnis w√§re, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je gr√∂√üer der Pr√§diktorwert ist, f√ºr den wir die abh√§ngige Variable sch√§tzen wollen.

Das klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen `check_heteroscedasticity()` aus dem `performance`-Package k√∂nnen wir sehr einfach pr√ºfen, ob diese Annahme verletzt wurde. Wir m√ºssen diese Funktion lediglich auf unser gesch√§tzes Regressionsmodell anwenden:

```{r}
check_heteroscedasticity(model)
```

Die Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur √úberp√ºfung der Annahme macht: Bei gr√ºner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p w√§re dann nicht signifikant. Bei roter Schrift - wie im vorliegenden Fall - ist die Fehlervarianz heteroskedastisch und p ist signifikant (p \< 0.05). Dann liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je gr√∂√üer der Pr√§diktorwert ist, f√ºr den wir die abh√§ngige Variable sch√§tzen wollen. Das m√ºssen wir dann bei der Interpretation der Daten ber√ºcksichtigen. Es ist dann auch unbedingt geboten, diese Limitation anzugeben.

Wie das ganze aussieht, k√∂nnen wir uns auch grafisch √ºber die `plot()`-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enth√§lt:

```{r}
plot(model, 1)
plot(fitted.values(model), rstandard(model))
```

Zun√§chst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier k√∂nnen wir bereits sehen, dass eine Zunahme der Streuung bei mittleren Werten erkennbar ist, weil wir einen leicht zur Mitte ge√∂ffneten Trichter haben. Das zweite Diagramm hilft zus√§tzlich mit einer roten Linie, die bei Homoskedastizit√§t m√∂glichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, k√∂nnen wir von Heteroskedastizit√§t ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!

Im Beispiel sehen wir aber auch, dass die Abweichung bei uns zwar vorhanden, aber nicht allzu "dramatisch" ist. Die Voraussetzung ist zwar verletzt, und das m√ºssen wir auch ber√ºcksichtgen, aber die Regressionsanalyse ist sehr robust gegen die Verletzung ihrer Voraussetzungen. Wir k√∂nnen den Fehler zudem korrigieren! Und das werden wir nun auch tun :)

### Was tun bei Heteroskedastizit√§t der Residuen? Berechnung von HC-Standard Errors!

Liegt *Heteroskedastizit√§t* vor, m√ºssen wir nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robust gegen die Verletzung ihrer Voraussetzungen. Zweitens k√∂nnen wir diesen Konflikt einigerma√üen elegant aufl√∂sen, indem wir pauschal *robuste Standardfehler* sch√§tzen lassen, so dass die Verletzung nicht mehr zu Sch√§tzfehlern f√ºhrt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizit√§t zu kontern. Eine einfache L√∂sung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl `vcov()`, der zur Berechnung von *heteroscedasticity consistent (HC) standard errors* f√ºhrt. So erm√∂glichen wir die Berechnung von *heteroskedastizit√§tskonsistenten bzw. heteroskedastizit√§tsrobusten Sch√§tzern*. Nutzen wir diese L√∂sung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Sch√§tzung.

PS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zun√§chst Typ 3, die auch Hayes & Cai (2007) empfehlen. HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir sp√§ter sehen werden, ist das bei uns leider auch der Fall)

```{r}
coeftest(model, vcov = vcovHC(model, type = "HC3"))

# diese Variante w√§hlen, wenn Residuen nicht normalverteilt sind 
#coeftest(model, vcov = vcovHC(model, type = "HC4")) 
```

Nach der Ausf√ºhrung erhalten wir eine neue Regressionstabelle. Wenn wir diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen wir, dass sich die eigentlichen Koeffizienten (*Estimates*) nicht ver√§ndert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (*Std. Error*), t-Werte und p-Werte. Diese sind nun um unsere Sch√§tzfehler durch Heteroskedastizit√§t korrigiert.

Also weiter geht's!

## Unabh√§ngigkeit der Residuen

Auch die Annahme, dass die *Residuen unabh√§ngig* voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabh√§ngigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm f√ºr eine bestimmte Beobachtung kenne, darf mir das keine Information √ºber den Fehlerterm f√ºr die n√§chste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) f√ºhren. Ansonsten l√§ge eine *Autokorrelation der Fehlerterme* vor, die die Aussagekraft des Modells reduzieren w√ºrde.

Das performance-package ist einfach soooo cool! Es beinhaltet auch die `check_autocorrelation()`-Funktion, mit der wir diese Annahme sehr einfach pr√ºfen k√∂nnen:

```{r}
check_autocorrelation(model)
```

Auch hier ist der Output wieder sehr klar: Die Pr√ºfung ergibt, dass die Residuen unabh√§ngig und nicht autokorreliert sind (p = 0.588) - sonst h√§tten wir auch hier einen signifikanten p-Wert erhalten. Prima!

## Normalverteilung der Residuen

Wenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene "Muster" in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Pr√§diktoren im Modell ber√ºcksichtigt haben und somit ein Teil der erkl√§renden Information in die Residuen √ºbergeht, wo sie das erkennbare Muster "verursacht".

Auch die Voraussetzung, dass die Residuen normalverteilt sein sollen, l√§sst sich mit einer Funktion aus dem performance-Package sehr einfach √ºberpr√ºfen:

```{r}
check_normality(model)
```

Auch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine "direkte Ansage" macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das m√ºssen wir bei der Interpretation der Daten ber√ºcksichtigen. Grunds√§tzlich k√∂nnen wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein *Bootstrapping-Verfahren* auf unsere Daten anwenden. Das aber nur zur Info, wenn ihr hier selbstst√§ndig weitermachen wollt - das w√ºrde jetzt etwas zu weit f√ºhren :) Au√üerdem werden wir unten bei der zus√§tzlichen visuellen Inspektion mit der Funktion `check_models()` auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist.

## Ausrei√üer im Modell

*Ausrei√üer* in den Daten sind ein Problem f√ºr viele parametrische Verfahren, denn einzelne Ausrei√üer k√∂nnen einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausrei√üer gibt, k√∂nnen wir wieder mit einer sehr einfachen Funktion aus dem performance-Package pr√ºfen, die auf das sogenannte "cooks distance" zur√ºckgreift. Der Wert gibt mir Auskunft dar√ºber, welchen Einfluss m√∂gliche Ausrei√üer auf das Modell haben.

```{r}
check_outliers(model)
```

In unserem Fall gibt es keine Ausrei√üer, die das Modell beintr√§chtigen - vielleicht h√§tten wir sonst auch keinen signifikanten Zusammenhang beobachten k√∂nnen.

::: {.callout-note collapse="true" icon="false"}
## Exkurs Visuelle Inspektion der Modellg√ºte bzw. der Modellannahmen

Es gibt im performance-Package auch eine sehr gehaltvolle Funktion, die uns eine visuelle Inspektion der Modellg√ºte bzw. verschiedenen Modellannahmen erlaubt (Normalit√§t der Residuen, Normalit√§t der zuf√§lligen Effekte, lineare Beziehung, Homogenit√§t der Varianz, Multikollinearit√§t). Mit der check_model-Funktion k√∂nnen wir uns dazu mehrere Grafiken im √úberblick ausgeben lassen. YEAH! :)

Hier der Link zur Dokumentation des Performance-Packages mit weiteren Informationen: [Link](https://easystats.github.io/performance/)

```{r}
#check_model(model)
#model_performance(model)
```
:::

::: callout-note
## Literatur

> üìñ Hayes, A. F.,& & Cai, L. (2007). Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722
:::
